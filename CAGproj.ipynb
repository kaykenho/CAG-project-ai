{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8b0b23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ff63a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load GPT-2 pre-trained model and tokenizer from Hugging Face\n",
    "model_name = \"gpt2\"  # Using the smaller model (gpt2)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Initialize tokenizer with padding on the left side\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n",
    "\n",
    "# Set pad_token to eos_token if not set already\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Set the model to evaluation mode (this disables dropout and other training behaviors)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "de1a870b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the cache with a maximum size\n",
    "cache_size = 5  # Maximum number of cache entries\n",
    "cache = deque(maxlen=cache_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27d7ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add content to the cache\n",
    "def add_to_cache(text, cache):\n",
    "    cache.append(text)\n",
    "\n",
    "# Function to retrieve relevant content from the cache\n",
    "def retrieve_from_cache(query, cache, k=3):\n",
    "    return list(cache)[-k:]\n",
    "\n",
    "# Modified generate function with attention mask and padding\n",
    "def generate_with_cache(query, cache, model, tokenizer, max_length=1024, max_new_tokens=50):\n",
    "    # Retrieve relevant cached information\n",
    "    relevant_texts = retrieve_from_cache(query, cache)\n",
    "    \n",
    "    # Combine the relevant cache text with the current query to form the complete context\n",
    "    context = \" \".join(relevant_texts) + \" \" + query\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode(context, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    \n",
    "    # Ensure the attention mask is set\n",
    "    attention_mask = torch.ones(inputs.shape, dtype=torch.long)\n",
    "    \n",
    "    # Ensure the total input length is within the model's max length\n",
    "    if inputs.shape[1] > max_length:\n",
    "        inputs = inputs[:, -max_length:]\n",
    "        attention_mask = attention_mask[:, -max_length:]\n",
    "    \n",
    "    # Check if there are any invalid token ids (out of vocab range)\n",
    "    if torch.any(inputs >= model.config.vocab_size):\n",
    "        raise ValueError(\"One or more tokens are out of vocabulary range.\")\n",
    "    \n",
    "    # Generate the output using GPT-2\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_new_tokens=max_new_tokens, attention_mask=attention_mask, pad_token_id=tokenizer.pad_token_id, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "    \n",
    "    # Decode the generated response\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8b9e705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding some example text to the cache\n",
    "add_to_cache(\"Artificial intelligence (AI) refers to the simulation of human intelligence in machines.\", cache)\n",
    "add_to_cache(\"The Turing test is used to evaluate a machine's ability to exhibit intelligent behavior.\", cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "18f0662f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kayky\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\kayky\\anaconda3\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me more about artificial intelligence.\n",
      "\n",
      "I'm a computer scientist at the University of California, Berkeley. I'm also a professor of computer science at Stanford University. And I've been working on artificial-intelligence for a long time. In fact, I was one of the first to write about it in a paper published in the journal Nature. But I didn't know about the topic until I read the paper. It's a very interesting paper, and I think it's important to understand how it works. The paper is called \"The Problem of Artificial Intelligence.\"\n",
      "...\n",
      " (1) The problem of artificial intelligences is that they are not just machines, but also people. They are people who are able to do things that are impossible. (2) They can do anything that is impossible, even if they have no knowledge of it. This is a problem that we have to solve. We have a lot of problems with artificial intelligent systems. One of them is the problem with the human mind. If you have an artificial brain, you can't do any of these things. You can only do one thing. So, if you want to know what is going on in your brain and what you are doing, then you need to learn to think about what's going to happen. That's what we call the \"problem of AI.\" (3) We can learn how to use the brain to make decisions. Now, we can also learn about how we think. What we do is we learn what the mind is doing. When we are thinking about something, the computer will tell us what to say. Then, when we're thinking of something else, it will say something about that. For example, a person who is thinking is saying, \"I want a cup of coffee.\" But if we want coffee, what do we say? We say, What is coffee? What does it taste like? And then, in order to get that cup, our brain will ask us to imagine what it is like to be a coffee addict. Our brain is not going, 'Oh, that's not good.' It is, instead, thinking, which is what makes it so difficult to control our thoughts. There are many ways to deal with this problem. First, there are the ways that the machine can think, such as, say we know that a certain person is in love with a particular person. Or we could say that there is something wrong with our body. These are all ways of dealing with it, because we don't have the ability to tell the difference between what a machine thinks and how a human thinks. Second, some of our problems are very complex. Some of us are really good at thinking. Others are just not. Sometimes we just don. Other times, sometimes we get it wrong. Maybe we should just stop thinking and start thinking again. Third, many of those problems can be solved by using the same kind of machine. A machine is an intelligent machine that can understand what people are saying. People are talking to each other. All of a sudden, they're talking about a different person, or a friend. How do you know if that person's talking? If they say yes, how do they know? How can you tell if someone is talking with you? The machine will try to answer that question. Fourth, people can talk to machines. Machines can tell you what they want you to hear. Suppose you're in an office. Your boss is sitting in front of you. He's saying something like, You know, this is really important. Well, he's just saying that because he wants to talk about this. Why would you say this? You're not really in that office, are you, so you don' know. Do you really want him to ask you this question? Do he really think that you should say it? Or do he just want the conversation to go on? It doesn't matter. Just say what he says. Fifth, machines can make mistakes. Most of all, machine learning is about learning. Machine learning can help you learn. Because it can teach you things about your mind, about thinking in general, like how you think and feel. Sixth, computers can solve problems. Computers can figure out how people think in real time, by looking at their faces. Computer scientists have been studying this for years. Even though they don`t know how they do it yet, computer scientists are working with computers to figure it out. Seventh, artificial minds can create problems that humans can not solve, for example. Artificial minds are machines that have learned to recognize and solve certain problems, including problems of language. Those problems have become more complex, more difficult, as we've learned. As we learned, these problems became more complicated. Finally, human beings can use artificial mind to help solve these kinds of things, to create new problems for themselves.\n"
     ]
    }
   ],
   "source": [
    "# Improved prompt to get a more relevant answer\n",
    "query = \"Tell me about the history and future of artificial intelligence, its applications, and current advancements.\"\n",
    "\n",
    "# Generating response with additional parameters to control quality\n",
    "outputs = model.generate(input_ids, \n",
    "                         max_length=1024,  # Set total length including prompt\n",
    "                         no_repeat_ngram_size=2,  # Avoid repeating n-grams\n",
    "                         temperature=0.7,  # Control randomness\n",
    "                         top_p=0.9,  # Use nucleus sampling for better variety\n",
    "                         top_k=50,  # Limit next token choices\n",
    "                         attention_mask=attention_mask, \n",
    "                         pad_token_id=tokenizer.pad_token_id)\n",
    "\n",
    "# Decode and display the result\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9acd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
